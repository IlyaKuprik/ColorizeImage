{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "colorizator_V2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRFlsDGrRAdA",
        "outputId": "d51c5708-4a2f-4807-cbbb-6e68130959d8"
      },
      "source": [
        "import keras\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.layers import Layer, BatchNormalization\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input\n",
        "from tensorflow.keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape\n",
        "from keras.layers import merge, concatenate\n",
        "from tensorflow.keras.layers import Activation, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.callbacks import TensorBoard \n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from keras.layers.core import RepeatVector, Permute\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave\n",
        "from keras.callbacks import *\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "print(\"Libriaries loaded\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libriaries loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxScTgNZRy7Q"
      },
      "source": [
        "photos = zipfile.ZipFile(\"/content/drive/MyDrive/buildings_and_human_dataset.zip\",'r')\n",
        "photos.extractall(\"input/\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF2FW8RSO41G"
      },
      "source": [
        "def create_inception_embedding(grayscaled_rgb):\n",
        "    grayscaled_rgb_resized = []\n",
        "    for i in grayscaled_rgb:\n",
        "        i = resize(i, (299, 299, 3), mode='constant')\n",
        "        grayscaled_rgb_resized.append(i)\n",
        "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
        "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
        "    embed = inception.predict(grayscaled_rgb_resized)\n",
        "    return embed\n",
        "\n",
        "    \n",
        "def load_train_data(path):\n",
        "    train_data = np.array([img_to_array(load_img(path + file_name)) for file_name in os.listdir(path)], dtype=float)\n",
        "    print(\"Train data is loaded.\")\n",
        "    print(len(train_data), \"img loaded.\")\n",
        "    train_data = 1.0 / 255 * train_data\n",
        "    return train_data\n",
        "\n",
        "\n",
        "def load_test_data(path):\n",
        "    test_data = np.array([img_to_array(load_img(path + file_name)) for file_name in os.listdir(path)], dtype=float)\n",
        "    gray_me = gray2rgb(rgb2gray(1.0/255*test_data))\n",
        "    X_embed = create_inception_embedding(gray_me)\n",
        "    X_test = rgb2lab(test_data)[:, :, :, 0]\n",
        "    X_test = X_test.reshape(X_test.shape + (1,))\n",
        "    Y_test = rgb2lab(test_data)[:, :, :, 1:]\n",
        "    Y_test /= 128\n",
        "    return X_test, X_embed, Y_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQL1NBVEV1m7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb07aeec-e7c4-4be8-bc3b-0d80311872a9"
      },
      "source": [
        "inception = InceptionResNetV2(weights='imagenet', include_top=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5\n",
            "225214464/225209952 [==============================] - 5s 0us/step\n",
            "225222656/225209952 [==============================] - 5s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snjyDH2VSE4G"
      },
      "source": [
        "path = \"/content/input/\"\n",
        "train_data = load_train_data(path + \"train/\")\n",
        "X_test, X_embed, Y_test = load_test_data(path + \"test/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjNBNeVRPKNT"
      },
      "source": [
        "embed_input = Input(shape=(1000,))\n",
        "\n",
        "#Encoder\n",
        "encoder_input = Input(shape=(None, None, 1,))\n",
        "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
        "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "\n",
        "#Fusion\n",
        "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
        "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
        "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
        "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) \n",
        "\n",
        "#Decoder\n",
        "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "\n",
        "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krda3UYnW0N2"
      },
      "source": [
        "class CustomSaver(Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (epoch + 1) % 10 == 0:  # or save after some epoch, each k-th epoch etc.\n",
        "            self.model.save_weights(f\"/content/drive/MyDrive/model_weights/human_and_buildings/model_v2_{epoch + 1}.h5\")\n",
        "        tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc-hCrwNCj8w"
      },
      "source": [
        "model.load_weights(\"/content/drive/MyDrive/model_v2_138.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrRjIsrxLqCP"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        rotation_range=20,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "batch_size = 40\n",
        "epochs_size = 200\n",
        "\n",
        "def image_a_b_gen(batch_size):\n",
        "    for batch in datagen.flow(train_data, batch_size=batch_size):\n",
        "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
        "        lab_batch = rgb2lab(batch)\n",
        "        X_batch = lab_batch[:,:,:,0]\n",
        "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
        "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
        "        yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hgIXjXoLPTMR"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "saver = CustomSaver()\n",
        "history = History()\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics = [\"accuracy\"])\n",
        "model.fit_generator(image_a_b_gen(batch_size),\n",
        "                    epochs=epochs_size,\n",
        "                    callbacks=[saver, history],\n",
        "                    steps_per_epoch= len(train_data) // batch_size,\n",
        "                    validation_data=([X_test, X_embed],Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-CVBjOqEISL"
      },
      "source": [
        "model.save_weights(f\"/content/drive/MyDrive/model_weights/human_and_buildings/model_v2{epochs_size}}.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLitBXZgfg49"
      },
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "sns.lineplot(data = {'accuracy':history.history['accuracy'], 'val_accuracy' : history.history['val_accuracy']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-T3bpj9dvRd"
      },
      "source": [
        "def colorize_part(img, path, shape = (256, 256)):\n",
        "  image = img_to_array(img)\n",
        "  color_me = [image]\n",
        "  color_me = np.array(color_me, dtype=float)\n",
        "  gray_me = gray2rgb(rgb2gray(1.0/255*color_me))\n",
        "  color_me_embed = create_inception_embedding(gray_me)\n",
        "  color_me = rgb2lab(1.0/255*image)[:,:,0]\n",
        "  color_me = color_me.reshape((1,*shape,1))\n",
        "\n",
        "  out = model.predict([color_me, color_me_embed])\n",
        "  out *= 128\n",
        "  cur = np.zeros((*shape , 3))\n",
        "  cur[:,:,0] = color_me[0][:,:,0]\n",
        "  cur[:,:,1:] = out[0]\n",
        "  cur = lab2rgb(cur)\n",
        "  return cur"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Nv2DUs3d8UC"
      },
      "source": [
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "\n",
        "\n",
        "def split_image(img, stride=256):\n",
        "    shape = img.size\n",
        "    part_list = []\n",
        "    for x in range(0, shape[0], stride):\n",
        "        img_line = []\n",
        "        for y in range(0, shape[1], stride):\n",
        "            part = img.crop((x, y, x + 256, y + 256))\n",
        "            img_line.append(part)\n",
        "        part_list.append(img_line)\n",
        "    return part_list\n",
        "\n",
        "def get_composed_arr(img_list, original_shape, stride):\n",
        "    composed_arr = np.zeros((original_shape[1], original_shape[0], 3))\n",
        "    mask = np.zeros((original_shape[1], original_shape[0], 3))\n",
        "    \n",
        "    max_x = original_shape[1]\n",
        "    max_y = original_shape[0]\n",
        "\n",
        "    pbar = tqdm(total=len(img_list)*len(img_list[0]), position=0, leave=True)\n",
        "    x = 0\n",
        "    count = 0\n",
        "    for i in range(len(img_list[0])):\n",
        "        y = 0\n",
        "        for j in range(len(img_list)):\n",
        "            pbar.update(1)\n",
        "            step_x = min(x + 256, max_x)\n",
        "            step_y = min(y + 256, max_y)\n",
        "            composed_arr[x:step_x, y:step_y, :] += img_list[j][i][:step_x-x, :step_y-y, :]\n",
        "            mask[x:step_x, y:step_y, :] += 1\n",
        "            y += stride\n",
        "\n",
        "            count += 1\n",
        "            pbar.set_description(f'\\tComposed {count} parts')\n",
        "        x += stride\n",
        "    pbar.close()\n",
        "    composed_arr /= mask\n",
        "    return composed_arr\n",
        "\n",
        "def compose_images(img_list, original_shape, stride=256):\n",
        "    composed_arr = get_composed_arr(img_list, original_shape, stride)\n",
        "    composed_img = Image.fromarray((composed_arr * 255).astype(np.uint8))\n",
        "    return composed_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFGMxTf6eDFh"
      },
      "source": [
        "def colorize_image(img_path, save_path, stride=256):\n",
        "    image = load_img(img_path)\n",
        "    splited_img = split_image(image, stride=stride)\n",
        "\n",
        "    print(f\"Coloring {img_path.split('/')[-1]}\")\n",
        "    print('\\tColoring parts...')\n",
        "    \n",
        "    count = 0\n",
        "    pbar = tqdm(total=len(splited_img)*len(splited_img[0]), position=0, leave=True)\n",
        "    for i in range(len(splited_img)):\n",
        "        for j in range(len(splited_img[0])):\n",
        "            splited_img[i][j] = colorize_part(splited_img[i][j], img_path)\n",
        "            \n",
        "            pbar.update(1)\n",
        "            count += 1\n",
        "            pbar.set_description(f\"\\tColorized {count} parts\")\n",
        "    pbar.close()\n",
        "    print('\\tCompose parts...')\n",
        "    composed = compose_images(splited_img, image.size, stride=stride)\n",
        "    print(img_path.split('/')[-1], f\"colorized with stride {stride}.\")\n",
        "    if not os.path.exists(save_path):\n",
        "      os.mkdir(save_path)\n",
        "    composed.save(f\"{save_path}/{stride}_\" + img_path.split('/')[-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vB-eZn6uBhH"
      },
      "source": [
        "bw_image_path = '/content/img/'\n",
        "\n",
        "for file in os.listdir(bw_image_path):\n",
        "  if file == '.ipynb_checkpoints':\n",
        "    continue\n",
        "  colorize_image(bw_image_path + file, '/content/result/', stride=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U7RhNlEsZzT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3139e4d-1cc6-4dc9-8dc9-1c88a5e0981c"
      },
      "source": [
        "path_lst = ['/content/2665.jpg']\n",
        "\n",
        "for path in path_lst:\n",
        "  for stride in [256, 32]:\n",
        "    colorize_image(path, '/content/result/', stride=stride)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coloring 2665.jpg\n",
            "\tColoring parts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tColorized 16 parts: 100%|██████████| 16/16 [00:06<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tCompose parts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tComposed 16 parts: 100%|██████████| 16/16 [00:00<00:00, 213.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2665.jpg colorized with stride 256.\n",
            "Coloring 2665.jpg\n",
            "\tColoring parts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tColorized 725 parts: 100%|██████████| 725/725 [04:23<00:00,  2.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tCompose parts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tComposed 725 parts: 100%|██████████| 725/725 [00:02<00:00, 283.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2665.jpg colorized with stride 32.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrOnfGyJ1Bfy",
        "outputId": "588892db-6d5a-4f29-ecdf-96d9becd28f9"
      },
      "source": [
        "  !zip -r /content/result.zip /content/result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/result/ (stored 0%)\n",
            "  adding: content/result/32_1 (1).png (deflated 1%)\n",
            "  adding: content/result/32_1 (2).png (deflated 1%)\n",
            "  adding: content/result/32_1 (3).png (deflated 0%)\n",
            "  adding: content/result/32_1 (4).png (deflated 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA_ODBZ1dx2-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}